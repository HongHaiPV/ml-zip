"""
Classes for prediction by partial matching estimators.
"""
from __future__ import annotations

import logging
import pickle
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import pickle as pkl
import numpy as np
from typing import Sequence, Any, Tuple

import utils
from frequency_estimators import Estimator

torch.set_printoptions(precision=10)
torch.set_default_tensor_type(torch.DoubleTensor)

logging.basicConfig(filename='training.log',
                    filemode='w',
                    format='%(name)s - %(levelname)s - %(message)s'
                    )
LOGGER = logging.getLogger()
LOGGER.setLevel(logging.INFO)


class PPMEstimator(Estimator):
  """
  Generic class for PPM estimators, take in the language model class and
  the model's configs.
  """

  def __init__(self, stream_type: str,
               context_width: int,
               model_type: Any,
               model_configs: dict) -> None:

    super().__init__()
    self.stream_type = stream_type
    self.context_width = context_width
    self.model_configs = model_configs
    self.model_type = model_type
    self.model = None
    self.padding = model_configs.get('padding', '<PAD>')

  def get_context(self, stream: Sequence[Any], index: int) -> None:
    """
    Generate the context as a rolling window. At the index i, the context is
    stream[i-context_width-1:i]. Pad the context with a special self.padding
    character if there

    Args:
      stream: The stream of data, generated by self.get_stream().
      index: The index of the last symbol in the current context window.

    Returns:
      None.
    """

    context = utils.rolling_window_context(stream, self.context_width,
                                           self.padding, index + 1)
    context_ids = [self.indices[i] for i in context]
    self.cdf = self.model.get_cdf(context_ids)

  def load_train_data(self, stream: Sequence, stream_length: int)\
      -> Tuple[Sequence, Sequence]:
    """
    Turn stream of data into training data for the language model. It is
    meant to be ML library agnostic.

    Args:
      stream: The stream of data, generated by self.get_stream().
      stream_length: The length of the data stream, generated by
      self.get_stream().

    Returns:
      contexts: List of rolling windows, each is the list of symbols' id in
        that window.
      targets: List of rolling windows, each is the list of symbols' id in
        that window, offset by 1 compared to the corresponding window in
        the contexts.
    """

    windows = [utils.rolling_window_context(stream, self.context_width,
                                            self.padding, i) for i in
               range(stream_length + 1)]

    contexts = [[self.indices[s] for s in context] for context in
                windows[:-1]]
    targets = [[self.indices[s] for s in label] for label in windows[1:]]
    return contexts, targets

  def get_stream(self, data: Sequence) -> Sequence:
    """
    Using the text stream function, split data based on characters or words.

    Args:
      data: The original data that need to be encoded.

    Returns:
      A sequence of symbols.
      The length of that sequence.
    """

    return utils.get_stream_text(data, mode=self.stream_type)

  def save(self, path: str) -> None:
    """
    Save the class instance to file.

    Args:
      path: The path of the saved file.

    Returns:
      None.
    """

    with open(path + '.pkl', 'wb') as out:
      pkl.dump(self, out, pickle.HIGHEST_PROTOCOL)

  @classmethod
  def from_saved(cls, path: str) -> PPMEstimator:
    """
    Load the class instance from file.

    Args:
      path: The path of the saved file.

    Returns:
      The loaded class instance.
    """

    with open(path + '.pkl', 'rb') as inp:
      return pkl.load(inp)

  def fit(self, data: Sequence):
    """
    Train the language model on the data.

    Args:
      data: The data that needed to compress.

    Returns:
      None.
    """
    stream, length = self.get_stream(data)
    self.symbols = sorted(set(stream) | {self.padding})
    self.num_symbols = len(self.symbols)
    self.indices = {s: idx for idx, s in enumerate(self.symbols)}
    self.model_configs['vocab_size'] = self.num_symbols
    self.model = self.model_type(self.model_configs)
    self.model.vocab_size = self.num_symbols
    contexts, targets = self.load_train_data(stream, length)
    self.model.fit(contexts, targets)

  def mode(self, mode: str) -> None:
    """
    Reset the CDF table each time called by resetting the hidden state of the
    model.

    Args:
      mode: Either 'encode' or 'decode'.

    Returns:
      None
    """
    if mode == 'encode':
      LOGGER.info('Encoding...')
    if mode == 'decode':
      LOGGER.info('Decoding...')
    self.model.reset_states()



class LSTM(nn.Module):
  """
  LSTM language model, implemented using PyTorch.
  """

  def __init__(self, configs: dict) -> None:
    super().__init__()
    self.device = torch.device('cuda') \
      if torch.cuda.is_available() else torch.device('cpu')

    # Init model's configs
    self.embed_size = configs.get('embed_size', 128)
    self.hidden_size = configs.get('hidden_size', 128)
    self.num_layers = configs.get('num_layers', 2)
    self.vocab_size = configs.get('vocab_size', 256)
    self.context_width = configs.get('context_width', 128)
    self.batch_size = configs.get('batch_size', 128)
    self.epochs = configs.get('epochs', 120)
    self.lr = configs.get('lr', 1e-4)

    # Model's structure
    self.embed = nn.Embedding(self.vocab_size, self.embed_size)
    self.lstm = nn.LSTM(input_size=self.embed_size,
                        hidden_size=self.hidden_size,
                        num_layers=self.num_layers,
                        dropout=0)
    self.linear = nn.Linear(self.hidden_size, self.vocab_size)

    # Init hidden state
    self.state_h, self.state_c = self.init_state()

  def forward(self, inputs: torch.Tensor,
              prev_state: Tuple[torch.Tensor, torch.Tensor]):
    """
    Forward method for PyTorch nn.Module.

    Args:
      inputs: The data input of dims (self.batch_size, self.context_width)
      prev_state: The previous hidden states of dims (self.num_layers,
        self.context_width, self.hidden_size).

    Returns:
      log_prob: Log probability (softmax) of the symbols, of dims (
        self.batch_size, self.context_width, self.vocab_size)
      state: The h and c state, both of size (self.num_layers,
        self.context_width, self.hidden_size)
    """

    embeds = self.embed(inputs)
    output, state = self.lstm(embeds, prev_state)
    log_prob = F.log_softmax(self.linear(output), dim=-1)
    return log_prob, state

  def init_state(self) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    Initialize the hidden state to zeros at the beginning each epoch.
    The dimensions are (self.num_layers, self.context_width, self.hidden_size).

    Returns:
      (h0, c0): The initial hidden state h0 and c0.
    """

    return (torch.zeros(self.num_layers, self.context_width, self.hidden_size),
            torch.zeros(self.num_layers, self.context_width, self.hidden_size))

  def reset_states(self):
    """
    Reset the hidden state of the model.

    Returns:
      None.
    """

    self.state_h, self.state_c = self.init_state()

  def fit(self, contexts: Sequence[Sequence], targets: Sequence[Sequence])\
    -> None:
    """
    Train the language model.

    Args:
      contexts: The list rolling windows of contexts.
      targets: The list rolling windows of contexts, one step ahead of
        contexts, i.e. contexts[1] = targets[0]

    Returns:
      None.
    """

    n_samples = len(targets)
    losses = []
    loss_fn = nn.NLLLoss()
    optimizer = optim.Adam(self.parameters(), lr=self.lr)

    self.train()
    print('Training...')
    # for epoch in range(self.epochs):
    for epoch in utils.progressbar(range(self.epochs), 'Epoch: '):
      total_loss = 0
      state_h, state_c = self.init_state()
      for batch_ids in range(0, n_samples, self.batch_size):
        optimizer.zero_grad()

        batch_contexts = contexts[batch_ids: batch_ids + self.batch_size]
        batch_targets = targets[batch_ids: batch_ids + self.batch_size]

        contexts_tensor = (torch.tensor(batch_contexts, dtype=torch.long)
                           .to(self.device))
        targets_tensor = (torch.tensor(batch_targets, dtype=torch.long)
                          .to(self.device))
        log_probs, (state_h, state_c) = self.forward(contexts_tensor,
                                                     (state_h, state_c))
        state_h = state_h.detach()
        state_c = state_c.detach()

        loss = loss_fn(log_probs.transpose(1, 2), targets_tensor)
        loss.backward()

        optimizer.step()
        total_loss += loss.item()

        if batch_ids % (100 * self.batch_size) == 0:
          LOGGER.info('Epoch: {} Average Loss: {}'
                      .format(epoch, np.array(total_loss).mean()))
          # print()
      losses.append(total_loss)

  def get_cdf(self, context: Sequence) -> Sequence:
    """
    Returns the CDF each time called base on the context.

    Args:
      context: The list of symbols' ids in the window.

    Returns:
      The numpy array of CDF.
    """

    self.eval()
    context = torch.tensor([context]).to(self.device)
    log_prob, (self.state_h, self.state_c) = self(context,
                                                  (self.state_h, self.state_c))
    distribution = torch.exp(log_prob[0, -1]).cpu().data.numpy()
    cdf = np.cumsum(distribution)
    cdf = np.concatenate([[0], cdf])
    return cdf
