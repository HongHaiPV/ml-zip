<p><b>Arithmetic coding</b> (<b>AC</b>) is a form of <a href="/wiki/Entropy_encoding" class="mw-redirect" title="Entropy encoding">entropy encoding</a> used in <a href="/wiki/Lossless_data_compression" class="mw-redirect" title="Lossless data compression">lossless data compression</a>. Normally, a <a href="/wiki/String_(computer_science)" title="String (computer science)">string of characters</a> is represented using a fixed number of <a href="/wiki/Bit" title="Bit">bits</a> per character, as in the <a href="/wiki/American_Standard_Code_for_Information_Interchange" class="mw-redirect" title="American Standard Code for Information Interchange">ASCII</a> code. When a string is converted to arithmetic encoding, frequently used characters will be stored with fewer bits and not-so-frequently occurring characters will be stored with more bits, resulting in fewer bits used in total. Arithmetic coding differs from other forms of entropy encoding, such as <a href="/wiki/Huffman_coding" title="Huffman coding">Huffman coding</a>, in that rather than separating the input into component symbols and replacing each with a code, arithmetic coding encodes the entire message into a single number, an <a href="/wiki/Arbitrary-precision_arithmetic" title="Arbitrary-precision arithmetic">arbitrary-precision</a> fraction <i>q</i>, where <span class="nowrap">0.0 â‰¤ <i>q</i> &lt; 1.0</span>. It represents the current information as a range, defined by two numbers.<sup id="cite_ref-LiDrew2014_1-0" class="reference"><a href="#cite_note-LiDrew2014-1">&#91;1&#93;</a></sup> A recent family of entropy coders called <a href="/wiki/Asymmetric_numeral_systems" title="Asymmetric numeral systems">asymmetric numeral systems</a> allows for faster implementations thanks to directly operating on a single natural number representing the current information.<sup id="cite_ref-PCS2015_2-0" class="reference"><a href="#cite_note-PCS2015-2">&#91;2&#93;</a></sup>
</p>
<figure class="mw-halign-right" typeof="mw:File/Thumb"><a href="/wiki/File:Arithmetic_coding_example.svg" class="mw-file-description"><img src="//upload.wikimedia.org/wikipedia/commons/thumb/8/82/Arithmetic_coding_example.svg/400px-Arithmetic_coding_example.svg.png" decoding="async" width="400" height="283" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/8/82/Arithmetic_coding_example.svg/600px-Arithmetic_coding_example.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/8/82/Arithmetic_coding_example.svg/800px-Arithmetic_coding_example.svg.png 2x" data-file-width="1052" data-file-height="744" /></a><figcaption>An arithmetic coding example assuming a fixed probability distribution of three symbols "A", "B", and "C". Probability of "A" is 50%, probability of "B" is 33% and probability of "C" is 17%. Furthermore, we assume that the recursion depth is known in each step. In step one we code "B" which is inside the interval &#91;0.5,&#160;0.83): The binary number "0.10<i>x</i>" is the shortest code that represents an interval that is entirely inside &#91;0.5,&#160;0.83). "<i>x</i>" means an arbitrary bit sequence. There are two extreme cases: the smallest <i>x</i> stands for zero which represents the left side of the represented interval. Then the left side of the interval is dec(0.10)&#160;=&#160;0.5. At the other extreme, <i>x</i> stands for a finite sequence of ones which has the upper limit dec(0.11)&#160;=&#160;0.75. Therefore, "0.10<i>x</i>" represents the interval &#91;0.5,&#160;0.75) which is inside &#91;0.5,&#160;0.83). Now we can leave out the "0." part since all intervals begin with&#160;"0." and we can ignore the "<i>x</i>" part because no matter what bit-sequence it represents, we will stay inside &#91;0.5,&#160;0.75).</figcaption></figure>